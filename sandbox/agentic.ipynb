{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import logfire\n",
    "from pydantic import BaseModel\n",
    "import sys\n",
    "from pydantic_ai import Agent\n",
    "from dotenv import load_dotenv\n",
    "from pydantic_ai.models.openai import OpenAIModel\n",
    "from pydantic_ai.providers.openai import OpenAIProvider\n",
    "\n",
    "\n",
    "# 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\n",
    "logfire.configure(send_to_logfire='if-token-present')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "class MyModel(BaseModel):\n",
    "    city: str\n",
    "    country: str\n",
    "\n",
    "# API key (consider using an environment variable instead of hardcoding)\n",
    "api_key = os.getenv('OPENAI_API_KEY') or 'your-api-key-here'\n",
    "\n",
    "# Define the model\n",
    "model_name = 'gpt-4o'\n",
    "model = OpenAIModel(model_name, provider=OpenAIProvider(api_key=api_key))\n",
    "\n",
    "# Create the agent\n",
    "agent = Agent(model, output_type=MyModel, instrument=True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    result = agent.run_sync('The windy city in the US of A.')\n",
    "    \n",
    "    # Print the main output\n",
    "    print(result.output)\n",
    "    \n",
    "    # Get the usage information\n",
    "    usage = result.usage()\n",
    "    \n",
    "    # Print the specific information you requested\n",
    "    print(f\"Request tokens: {usage.request_tokens}\")\n",
    "    print(f\"Response tokens: {usage.response_tokens}\")\n",
    "    print(f\"LLM model used: {model_name}\")\n",
    "    \n",
    "    # If you want additional information\n",
    "    print(f\"Total tokens: {usage.total_tokens}\")\n",
    "    print(f\"Number of requests: {usage.requests}\")\n",
    "    \n",
    "    # If there are any model-specific details\n",
    "    if usage.details:\n",
    "        print(f\"Additional usage details: {usage.details}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNumber of requests: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00musage.requests\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py:191\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(main, debug, loop_factory)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[33;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    187\u001b[39m \u001b[33;03m    asyncio.run(main())\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    190\u001b[39m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    192\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug=debug, loop_factory=loop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m runner.run(main)\n",
      "\u001b[31mRuntimeError\u001b[39m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async def main():\n",
    "    result = await agent.run('The windy city in the US of A.')\n",
    "    \n",
    "    # Print the main output\n",
    "    print(result.output)\n",
    "    \n",
    "    # Get the usage information\n",
    "    usage = result.usage()\n",
    "    \n",
    "    # Print the specific information you requested\n",
    "    print(f\"Request tokens: {usage.request_tokens}\")\n",
    "    print(f\"Response tokens: {usage.response_tokens}\")\n",
    "    print(f\"LLM model used: {model_name}\")\n",
    "    \n",
    "    # Additional info\n",
    "    print(f\"Total tokens: {usage.total_tokens}\")\n",
    "    print(f\"Number of requests: {usage.requests}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:56:49.802 agent run\n",
      "19:56:49.804   chat gpt-4o\n",
      "city='Chicago' country='United States of America'\n",
      "--------\n",
      "Request tokens: 58\n",
      "Response tokens: 22\n",
      "Model provider: openai\n",
      "Model name: gpt-4o\n",
      "Cached tokens: 0\n",
      "Open telemetry: {'gen_ai.usage.input_tokens': 58, 'gen_ai.usage.output_tokens': 22}\n",
      "Agent name: agent\n",
      "--------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgentRunResult(output=MyModel(city='Chicago', country='United States of America'))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import logfire\n",
    "import asyncio\n",
    "from pydantic import BaseModel\n",
    "import sys\n",
    "from pydantic_ai import Agent\n",
    "from dotenv import load_dotenv\n",
    "from pydantic_ai.models.openai import OpenAIModel\n",
    "from pydantic_ai.providers.openai import OpenAIProvider\n",
    "\n",
    "# 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\n",
    "logfire.configure(send_to_logfire='if-token-present')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "class MyModel(BaseModel):\n",
    "    city: str\n",
    "    country: str\n",
    "\n",
    "# Use environment variable for API key\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "api_key = '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'  # Replace with your actual API key or use OPENAI_API_KEY\n",
    "\n",
    "# Define the model\n",
    "model_name = 'gpt-4o'\n",
    "model = OpenAIModel(model_name, provider=OpenAIProvider(api_key=api_key))\n",
    "\n",
    "# Create the agent\n",
    "agent = Agent(model, output_type=MyModel, instrument=True)\n",
    "\n",
    "async def main():\n",
    "    # Use the async run method instead of run_sync\n",
    "    result = await agent.run('The windy city in the US of A.')\n",
    "    \n",
    "    # Print the output\n",
    "    print(result.output)\n",
    "    \n",
    "    # Get and print the usage information\n",
    "    usage = result.usage()\n",
    "        \n",
    "    \n",
    "    model_info = agent.model.system\n",
    "    model_name = agent.model.model_name\n",
    "    cached_tokens = usage.details['cached_tokens']\n",
    "    open_telemetry = usage.opentelemetry_attributes()\n",
    "    \n",
    "    \n",
    "    print('--------')\n",
    "    print(f\"Request tokens: {usage.request_tokens}\")\n",
    "    print(f\"Response tokens: {usage.response_tokens}\")\n",
    "    print(f\"Model provider: {model_info}\")\n",
    "    print(f\"Model name: {model_name}\")\n",
    "    print(f\"Cached tokens: {cached_tokens}\")\n",
    "    print(f\"Open telemetry: {open_telemetry}\")\n",
    "    print(f\"Agent name: {agent.name}\")\n",
    "    print('--------')\n",
    "    \n",
    "\n",
    "    return result\n",
    "\n",
    "# For Jupyter/IPython environments, use:\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
