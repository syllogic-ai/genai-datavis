{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'services'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# sys.path.append(r\"..\\backend\\app\\services\")\u001b[39;00m\n\u001b[32m      3\u001b[39m sys.path.append(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m..apps\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mbackend\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mservices\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mai_service\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AIService\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# from ..apps.backend.app import main\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'services'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# sys.path.append(r\"..\\backend\\app\\services\")\n",
    "sys.path.append(r\"..apps\\backend\")\n",
    "from services.ai_service import AIService\n",
    "# from ..apps.backend.app import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.2.5-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.2.3-cp313-cp313-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Downloading numpy-2.2.5-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.2.5 pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1028)>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSSLCertVerificationError\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:1319\u001b[39m, in \u001b[36mAbstractHTTPHandler.do_open\u001b[39m\u001b[34m(self, http_class, req, **http_conn_args)\u001b[39m\n\u001b[32m   1318\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1319\u001b[39m     \u001b[43mh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[43m              \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTransfer-encoding\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1321\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:1338\u001b[39m, in \u001b[36mHTTPConnection.request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1337\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1338\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:1384\u001b[39m, in \u001b[36mHTTPConnection._send_request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1383\u001b[39m     body = _encode(body, \u001b[33m'\u001b[39m\u001b[33mbody\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1384\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:1333\u001b[39m, in \u001b[36mHTTPConnection.endheaders\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1332\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[32m-> \u001b[39m\u001b[32m1333\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:1093\u001b[39m, in \u001b[36mHTTPConnection._send_output\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffer[:]\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1096\u001b[39m \n\u001b[32m   1097\u001b[39m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:1037\u001b[39m, in \u001b[36mHTTPConnection.send\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_open:\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:1479\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1477\u001b[39m     server_hostname = \u001b[38;5;28mself\u001b[39m.host\n\u001b[32m-> \u001b[39m\u001b[32m1479\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1480\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:455\u001b[39m, in \u001b[36mSSLContext.wrap_socket\u001b[39m\u001b[34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[39m\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    450\u001b[39m                 do_handshake_on_connect=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    451\u001b[39m                 suppress_ragged_eofs=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    452\u001b[39m                 server_hostname=\u001b[38;5;28;01mNone\u001b[39;00m, session=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    453\u001b[39m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[32m    454\u001b[39m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msslsocket_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1076\u001b[39m, in \u001b[36mSSLSocket._create\u001b[39m\u001b[34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[39m\n\u001b[32m   1075\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1076\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1372\u001b[39m, in \u001b[36mSSLSocket.do_handshake\u001b[39m\u001b[34m(self, block)\u001b[39m\n\u001b[32m   1371\u001b[39m         \u001b[38;5;28mself\u001b[39m.settimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1372\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mSSLCertVerificationError\u001b[39m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1028)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mURLError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://ptsbrkwalysbchtdharj.supabase.co/storage/v1/object/public/test-bucket/mr46r49qgzi_1745310666295.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/common.py:728\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    725\u001b[39m     codecs.lookup_error(errors)\n\u001b[32m    727\u001b[39m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m728\u001b[39m ioargs = \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    736\u001b[39m handle = ioargs.filepath_or_buffer\n\u001b[32m    737\u001b[39m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/common.py:384\u001b[39m, in \u001b[36m_get_filepath_or_buffer\u001b[39m\u001b[34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[32m    383\u001b[39m req_info = urllib.request.Request(filepath_or_buffer, headers=storage_options)\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[32m    385\u001b[39m     content_encoding = req.headers.get(\u001b[33m\"\u001b[39m\u001b[33mContent-Encoding\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding == \u001b[33m\"\u001b[39m\u001b[33mgzip\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/common.py:289\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    283\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    284\u001b[39m \u001b[33;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[33;03mthe stdlib.\u001b[39;00m\n\u001b[32m    286\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01murllib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrequest\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:189\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(url, data, timeout, context)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    188\u001b[39m     opener = _opener\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:489\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    486\u001b[39m     req = meth(req)\n\u001b[32m    488\u001b[39m sys.audit(\u001b[33m'\u001b[39m\u001b[33murllib.Request\u001b[39m\u001b[33m'\u001b[39m, req.full_url, req.data, req.headers, req.get_method())\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[32m    492\u001b[39m meth_name = protocol+\u001b[33m\"\u001b[39m\u001b[33m_response\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:506\u001b[39m, in \u001b[36mOpenerDirector._open\u001b[39m\u001b[34m(self, req, data)\u001b[39m\n\u001b[32m    503\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m    505\u001b[39m protocol = req.type\n\u001b[32m--> \u001b[39m\u001b[32m506\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m                          \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m_open\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[32m    509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:466\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[32m    465\u001b[39m     func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    467\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    468\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:1367\u001b[39m, in \u001b[36mHTTPSHandler.https_open\u001b[39m\u001b[34m(self, req)\u001b[39m\n\u001b[32m   1366\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[32m-> \u001b[39m\u001b[32m1367\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1368\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_context\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:1322\u001b[39m, in \u001b[36mAbstractHTTPHandler.do_open\u001b[39m\u001b[34m(self, http_class, req, **http_conn_args)\u001b[39m\n\u001b[32m   1319\u001b[39m         h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[32m   1320\u001b[39m                   encode_chunked=req.has_header(\u001b[33m'\u001b[39m\u001b[33mTransfer-encoding\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m   1321\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[32m   1323\u001b[39m     r = h.getresponse()\n\u001b[32m   1324\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[31mURLError\u001b[39m: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1028)>"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"https://ptsbrkwalysbchtdharj.supabase.co/storage/v1/object/public/test-bucket/mr46r49qgzi_1745310666295.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import base64\n",
    "import io\n",
    "import plotly.io as pio\n",
    "import threading\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"input.csv\")\n",
    "ai_service = AIService()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>amount</th>\n",
       "      <th>subsidiary_name</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>125000</td>\n",
       "      <td>Alpha Corp</td>\n",
       "      <td>North America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-02-01</td>\n",
       "      <td>98000</td>\n",
       "      <td>Beta Ltd</td>\n",
       "      <td>Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>112000</td>\n",
       "      <td>Gamma Inc</td>\n",
       "      <td>Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>134000</td>\n",
       "      <td>Delta LLC</td>\n",
       "      <td>South America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>89000</td>\n",
       "      <td>Epsilon Co</td>\n",
       "      <td>Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2024-08-01</td>\n",
       "      <td>107000</td>\n",
       "      <td>Alpha Beta Gamma Delta</td>\n",
       "      <td>Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2024-09-01</td>\n",
       "      <td>126000</td>\n",
       "      <td>Beta Gamma Delta Epsilon</td>\n",
       "      <td>Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>151000</td>\n",
       "      <td>Gamma Delta Epsilon Zeta</td>\n",
       "      <td>South America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2024-11-01</td>\n",
       "      <td>95000</td>\n",
       "      <td>Delta Epsilon Zeta Theta</td>\n",
       "      <td>Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2024-12-01</td>\n",
       "      <td>162000</td>\n",
       "      <td>Epsilon Zeta Theta Iota</td>\n",
       "      <td>Australia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         month  amount           subsidiary_name         region\n",
       "0   2024-01-01  125000                Alpha Corp  North America\n",
       "1   2024-02-01   98000                  Beta Ltd         Europe\n",
       "2   2024-03-01  112000                 Gamma Inc           Asia\n",
       "3   2024-04-01  134000                 Delta LLC  South America\n",
       "4   2024-05-01   89000                Epsilon Co         Africa\n",
       "..         ...     ...                       ...            ...\n",
       "67  2024-08-01  107000    Alpha Beta Gamma Delta         Europe\n",
       "68  2024-09-01  126000  Beta Gamma Delta Epsilon           Asia\n",
       "69  2024-10-01  151000  Gamma Delta Epsilon Zeta  South America\n",
       "70  2024-11-01   95000  Delta Epsilon Zeta Theta         Africa\n",
       "71  2024-12-01  162000   Epsilon Zeta Theta Iota      Australia\n",
       "\n",
       "[72 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(data, query=None, context=None, ai_service=None):\n",
    "        validation_output = []\n",
    "        # validation_output.append(\"Validating your data...\")\n",
    "\n",
    "        # Check for missing values\n",
    "        if data.isnull().sum().sum() > 0:\n",
    "            validation_output.append(\"The following columns have empty values:\")\n",
    "            validation_output.append(data.isnull().sum().to_dict())\n",
    "        else:\n",
    "            validation_output.append(\"No empty values found!\")\n",
    "\n",
    "        # Check for duplicates\n",
    "        if data.duplicated().sum() > 0:\n",
    "            dup_num = data.duplicated().sum()\n",
    "            validation_output.append(f\"{dup_num} duplicate entries found.\")\n",
    "        else:\n",
    "            validation_output.append(\"No duplicate entries found.\")\n",
    "\n",
    "        # Validate data types\n",
    "        # validation_output.append(\"Validating the data types...\")\n",
    "        data['month'] = pd.to_datetime(data['month'], errors='coerce')\n",
    "        return validation_output\n",
    "\n",
    "def get_insights(data, query, context, ai_service):\n",
    "        insights_output = []\n",
    "        # print(\"Going for insights\")\n",
    "        data_text = data.to_string(index=False)\n",
    "        # context = f\"The following is the financial data for 2024:\\n{data_text}\"\n",
    "        final_query = f\"Generate valuable business insights about the given data: {data_text}, trying to best answer the following  query: {query}. \\\n",
    "        Be specific and provide actionable insights. \"\n",
    "        response = ai_service.process_query(context, final_query)\n",
    "        # print(response)\n",
    "        insights_output.append(response)\n",
    "\n",
    "        return insights_output\n",
    "\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "def exec_with_timeout(code, local_vars, timeout=5):\n",
    "    def target():\n",
    "        try:\n",
    "            exec(code, {}, local_vars)\n",
    "        except Exception as e:\n",
    "            local_vars[\"error\"] = e\n",
    "\n",
    "    thread = threading.Thread(target=target)\n",
    "    thread.start()\n",
    "    thread.join(timeout)\n",
    "\n",
    "    if thread.is_alive():\n",
    "        raise TimeoutException(\"Execution timed out\")\n",
    "    if \"error\" in local_vars:\n",
    "        raise local_vars[\"error\"]\n",
    "\n",
    "def visualize(data, query, context, ai_service):\n",
    "        if not query:\n",
    "            query = \"Given the data, provide me a valuable visualization\"\n",
    "        visualization_output = []\n",
    "        viz_components = {\n",
    "             \"barchart\": [\"x\", \"y\", \"color\", \"title\", \"xaxis_title\", \"yaxis_title\"],\n",
    "             \"piechart\": [\"values\", \"names\", \"title\"],\n",
    "                \"scatter\": [\"x\", \"y\", \"color\", \"title\", \"xaxis_title\", \"yaxis_title\"],\n",
    "                \"line\": [\"x\", \"y\", \"title\", \"xaxis_title\", \"yaxis_title\"],\n",
    "                \"heatmap\": [\"z\", \"x\", \"y\", \"title\", \"xaxis_title\", \"yaxis_title\"],\n",
    "        }\n",
    "\n",
    "        init_queries = [\n",
    "             f\"I want you to decide what's the best visualization type to use in order to answer the following question: {query} \\\n",
    "             Could be a barchart, piechart, scatter, line, or heatmap. \\\n",
    "                Print only the name of the visualization type and nothing else. \",\n",
    "\n",
    "                f\"Given the following data: {data.head(10).to_string(index=False)} I want you to give me the following components for a nice visualization \\\n",
    "                    Print only the components and nothing else in the following format: \\\n",
    "                    <component1>: <value1>, <component2>: <value2>, <component3>: <value3>, ...\\n \" \n",
    "    \n",
    "        ]\n",
    "        \n",
    "        data_text = data.to_string(index=False)\n",
    "        # context = f\"The following is the financial data for 2024:\\n{data_text}\"\n",
    "\n",
    "        # First LLM Pass to get the visualization type\n",
    "        query_type = ai_service.process_query(context, init_queries[0])\n",
    "\n",
    "        if query_type not in viz_components.keys(): \n",
    "             query_type = \"barchart\"\n",
    "\n",
    "        # Second LLM Pass to get the visualization components\n",
    "        query_components = ai_service.process_query(context, init_queries[1] + \"I am looking to creating a \" + query_type + \". The components I need: \" + str(viz_components[query_type]))\n",
    "        # Transform the query components into a dictionary\n",
    "        query_components = query_components.split(\",\") \n",
    "        query_components = [x.strip() for x in query_components]\n",
    "        query_components = {k: v.split(\": \")[1] for k, v in zip(viz_components[query_type], query_components)}\n",
    "\n",
    "        visualization_output.append(query_components)\n",
    "        \n",
    "        return visualization_output\n",
    "\n",
    "def calculate(data, query, context, ai_service):\n",
    "        calculation_output = []\n",
    "        data_columns = data.columns\n",
    "        final_query = f\"Given a table with the following metadata: {data_columns} \\\n",
    "              I want you to generate a SQL code, trying to answer the following question: {query}. \\\n",
    "              Assume that the table name is 'data_table'. \\\n",
    "              Provide the SQL code only, and nothing else.\"\n",
    "        response = ai_service.process_query(context, final_query)\n",
    "\n",
    "        # Create an in-memory SQLite database\n",
    "        conn = sqlite3.connect(\":memory:\")\n",
    "        print(f\"Calculate response: {response}\")\n",
    "        try:\n",
    "            # Load the DataFrame into the SQLite database\n",
    "            data.to_sql(\"data_table\", conn, index=False, if_exists=\"replace\")\n",
    "\n",
    "            # Execute the SQL query\n",
    "            result = pd.read_sql_query(response, conn)\n",
    "\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing SQL query: {e}\")\n",
    "            return None\n",
    "        finally:\n",
    "            # Close the connection\n",
    "            conn.close()\n",
    "\n",
    "        calculation_output.append(result)\n",
    "\n",
    "        return calculation_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_funcs = {\n",
    "    \"validate_data\": validate_data,\n",
    "    \"get_insights\": get_insights,\n",
    "    \"visualize\": visualize,\n",
    "    \"calculate\": calculate,\n",
    "}\n",
    "\n",
    "tools_explanation = {\n",
    "    \"validate_data\": \"Check for missing values, duplicates, and data types.\",\n",
    "    \"get_insights\": \"Generate valuable business insights about the given data.\",\n",
    "    \"visualize\": \"Generate a visualization of the data.\",\n",
    "    \"calculate\": \"Performs calculations based on a user question.\"\n",
    "}\n",
    "\n",
    "context = f\"You are an experienced data analyst, expert in giving quality information and insights about various data types. \\\n",
    "    I will be giving you a dataset, and you will be providing quality deiliverables. You have the following tools available: {{tools}}. \" \\\n",
    "\n",
    "master_query = f\"Given the following dataset: {{data}}, a user query {{user_query}}, I want you to decide which tool to use in order to answer the user query. \" \\\n",
    "\"Print only the name of the tool and nothing else. \" \n",
    "\n",
    "feedback_query = f\"Given the user query {{user_query}}, you have used the following tools, producing the respective responses: \\\n",
    "    {{tool_responses}} \\\n",
    "    Do you have enough information to answer the user query, or are additional information missing?  \\\n",
    "    If yes, print 'yes' and nothing else. \\\n",
    "    If not, print only the name of the tool you need to use in order to answer the user query, and nothing else. Don't price NO in this case. \\\n",
    "    Available tools are {{tools}}. \" \n",
    "\n",
    "response_query = f\"Given the user query {{user_query}}, you have used the following tools, producing the respective responses: \\\n",
    "    {{tool_responses}} \\\n",
    "    Using the above, please provide the user with a proper response. \\\n",
    "    Be as precise as possible, and only answer exactlt what is asked in the user query. \\\n",
    "    Don't mention any information about the tools you have used.\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Champion</th>\n",
       "      <th>Regular Season Wins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1980</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1981</td>\n",
       "      <td>Boston Celtics</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1982</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1983</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1984</td>\n",
       "      <td>Boston Celtics</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1985</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1986</td>\n",
       "      <td>Boston Celtics</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1987</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1988</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1989</td>\n",
       "      <td>Detroit Pistons</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1990</td>\n",
       "      <td>Detroit Pistons</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1991</td>\n",
       "      <td>Chicago Bulls</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1992</td>\n",
       "      <td>Chicago Bulls</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1993</td>\n",
       "      <td>Chicago Bulls</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1994</td>\n",
       "      <td>Houston Rockets</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1995</td>\n",
       "      <td>Houston Rockets</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1996</td>\n",
       "      <td>Chicago Bulls</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1997</td>\n",
       "      <td>Chicago Bulls</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1998</td>\n",
       "      <td>Chicago Bulls</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1999</td>\n",
       "      <td>San Antonio Spurs</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2000</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2001</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2002</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2003</td>\n",
       "      <td>San Antonio Spurs</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2004</td>\n",
       "      <td>Detroit Pistons</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2005</td>\n",
       "      <td>San Antonio Spurs</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2006</td>\n",
       "      <td>Miami Heat</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2007</td>\n",
       "      <td>San Antonio Spurs</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2008</td>\n",
       "      <td>Boston Celtics</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2009</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2010</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2011</td>\n",
       "      <td>Dallas Mavericks</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2012</td>\n",
       "      <td>Miami Heat</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2013</td>\n",
       "      <td>Miami Heat</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2014</td>\n",
       "      <td>San Antonio Spurs</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2015</td>\n",
       "      <td>Golden State Warriors</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2016</td>\n",
       "      <td>Cleveland Cavaliers</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2017</td>\n",
       "      <td>Golden State Warriors</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2018</td>\n",
       "      <td>Golden State Warriors</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2019</td>\n",
       "      <td>Toronto Raptors</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2020</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2021</td>\n",
       "      <td>Milwaukee Bucks</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2022</td>\n",
       "      <td>Golden State Warriors</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Year               Champion  Regular Season Wins\n",
       "0   1980     Los Angeles Lakers                   60\n",
       "1   1981         Boston Celtics                   62\n",
       "2   1982     Los Angeles Lakers                   57\n",
       "3   1983     Philadelphia 76ers                   65\n",
       "4   1984         Boston Celtics                   62\n",
       "5   1985     Los Angeles Lakers                   62\n",
       "6   1986         Boston Celtics                   67\n",
       "7   1987     Los Angeles Lakers                   65\n",
       "8   1988     Los Angeles Lakers                   62\n",
       "9   1989        Detroit Pistons                   63\n",
       "10  1990        Detroit Pistons                   59\n",
       "11  1991          Chicago Bulls                   61\n",
       "12  1992          Chicago Bulls                   67\n",
       "13  1993          Chicago Bulls                   57\n",
       "14  1994        Houston Rockets                   58\n",
       "15  1995        Houston Rockets                   47\n",
       "16  1996          Chicago Bulls                   72\n",
       "17  1997          Chicago Bulls                   69\n",
       "18  1998          Chicago Bulls                   62\n",
       "19  1999      San Antonio Spurs                   37\n",
       "20  2000     Los Angeles Lakers                   67\n",
       "21  2001     Los Angeles Lakers                   56\n",
       "22  2002     Los Angeles Lakers                   58\n",
       "23  2003      San Antonio Spurs                   60\n",
       "24  2004        Detroit Pistons                   54\n",
       "25  2005      San Antonio Spurs                   59\n",
       "26  2006             Miami Heat                   52\n",
       "27  2007      San Antonio Spurs                   58\n",
       "28  2008         Boston Celtics                   66\n",
       "29  2009     Los Angeles Lakers                   65\n",
       "30  2010     Los Angeles Lakers                   57\n",
       "31  2011       Dallas Mavericks                   57\n",
       "32  2012             Miami Heat                   66\n",
       "33  2013             Miami Heat                   54\n",
       "34  2014      San Antonio Spurs                   67\n",
       "35  2015  Golden State Warriors                   73\n",
       "36  2016    Cleveland Cavaliers                   67\n",
       "37  2017  Golden State Warriors                   58\n",
       "38  2018  Golden State Warriors                   53\n",
       "39  2019        Toronto Raptors                   52\n",
       "40  2020     Los Angeles Lakers                   56\n",
       "41  2021        Milwaukee Bucks                   51\n",
       "42  2022  Golden State Warriors                   53"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[155], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcalculate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_upd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mai_service\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[148], line 106\u001b[0m, in \u001b[0;36mcalculate\u001b[1;34m(data, query, context, ai_service)\u001b[0m\n\u001b[0;32m    101\u001b[0m data_columns \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m    102\u001b[0m final_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGiven a table with the following metadata: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_columns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124m      I want you to generate a SQL code, trying to answer the following question: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124m      Assume that the table name is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_table\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124m      Provide the SQL code only, and nothing else.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 106\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mai_service\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# Create an in-memory SQLite database\u001b[39;00m\n\u001b[0;32m    109\u001b[0m conn \u001b[38;5;241m=\u001b[39m sqlite3\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:memory:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\goust\\OneDrive\\Έγγραφα\\EARLY_YEARS\\Projects\\genai_analyst\\generative-ai-app\\analyses\\..\\backend\\app\\services\\ai_service.py:9\u001b[0m, in \u001b[0;36mAIService.process_query\u001b[1;34m(self, context, query)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, context: \u001b[38;5;28mstr\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Use the LLM tool to process the query\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_tool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\goust\\OneDrive\\Έγγραφα\\EARLY_YEARS\\Projects\\genai_analyst\\generative-ai-app\\analyses\\..\\backend\\app\\tools\\llm_tool.py:41\u001b[0m, in \u001b[0;36mLLMTool.generate_response\u001b[1;34m(self, context, query)\u001b[0m\n\u001b[0;32m     34\u001b[0m payload \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m: parameters\n\u001b[0;32m     37\u001b[0m     }\n\u001b[0;32m     40\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl, headers\u001b[38;5;241m=\u001b[39mheaders, json\u001b[38;5;241m=\u001b[39mpayload)\n\u001b[1;32m---> 41\u001b[0m response_text \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response_text\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "calculate(data, user_query, context_upd, ai_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What is the monthly revenue per region?\"\n",
    "user_query = \"What is the total revenue per month?\"\n",
    "user_query = \"Give me the 5 top teams with the highest average number of which when they won the champion.\"\n",
    "data = pd.read_csv(\"nba_champions_1980_onwards.csv\")\n",
    "# data = pd.read_csv(\"input.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = \"validate_data\"\n",
    "\n",
    "# tools[response](data, ai_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "Using tool: visualize\n",
      "Iteration 2\n",
      "Using tool: calculate\n",
      "Calculate response: SELECT Champion, AVG(RS_Wins) AS Average_RS_Wins\n",
      "FROM (\n",
      "    SELECT Champion, SUM(RS_Wins) AS RS_Wins\n",
      "    FROM data_table\n",
      "    GROUP BY Champion\n",
      ") AS subquery\n",
      "ORDER BY Average_RS_Wins DESC\n",
      "LIMIT 5;\n",
      "Error executing SQL query: Execution failed on sql 'SELECT Champion, AVG(RS_Wins) AS Average_RS_Wins\n",
      "FROM (\n",
      "    SELECT Champion, SUM(RS_Wins) AS RS_Wins\n",
      "    FROM data_table\n",
      "    GROUP BY Champion\n",
      ") AS subquery\n",
      "ORDER BY Average_RS_Wins DESC\n",
      "LIMIT 5;': no such column: RS_Wins\n",
      "Iteration 3\n",
      "Using tool: calculate\n",
      "Calculate response: SELECT Champion, AVG(RS_Wins) AS Average_RS_Wins\n",
      "FROM (\n",
      "    SELECT Champion, SUM(RS_Wins) AS RS_Wins\n",
      "    FROM data_table\n",
      "    GROUP BY Champion\n",
      ") AS subquery\n",
      "ORDER BY Average_RS_Wins DESC\n",
      "LIMIT 5;\n",
      "Error executing SQL query: Execution failed on sql 'SELECT Champion, AVG(RS_Wins) AS Average_RS_Wins\n",
      "FROM (\n",
      "    SELECT Champion, SUM(RS_Wins) AS RS_Wins\n",
      "    FROM data_table\n",
      "    GROUP BY Champion\n",
      ") AS subquery\n",
      "ORDER BY Average_RS_Wins DESC\n",
      "LIMIT 5;': no such column: RS_Wins\n",
      "Iteration 4\n",
      "Using tool: calculate\n",
      "Calculate response: SELECT Champion, AVG(RS_Wins) AS Average_RS_Wins\n",
      "FROM (\n",
      "    SELECT Champion, SUM(RS_Wins) AS RS_Wins\n",
      "    FROM data_table\n",
      "    GROUP BY Champion\n",
      ") AS subquery\n",
      "ORDER BY Average_RS_Wins DESC\n",
      "LIMIT 5;\n",
      "Error executing SQL query: Execution failed on sql 'SELECT Champion, AVG(RS_Wins) AS Average_RS_Wins\n",
      "FROM (\n",
      "    SELECT Champion, SUM(RS_Wins) AS RS_Wins\n",
      "    FROM data_table\n",
      "    GROUP BY Champion\n",
      ") AS subquery\n",
      "ORDER BY Average_RS_Wins DESC\n",
      "LIMIT 5;': no such column: RS_Wins\n",
      "Iteration 5\n",
      "Using tool: calculate\n",
      "Calculate response: SELECT Champion, AVG(RS_Wins) AS Average_RS_Wins\n",
      "FROM (\n",
      "    SELECT Champion, SUM(RS_Wins) AS RS_Wins\n",
      "    FROM data_table\n",
      "    GROUP BY Champion\n",
      ") AS subquery\n",
      "ORDER BY Average_RS_Wins DESC\n",
      "LIMIT 5;\n",
      "Error executing SQL query: Execution failed on sql 'SELECT Champion, AVG(RS_Wins) AS Average_RS_Wins\n",
      "FROM (\n",
      "    SELECT Champion, SUM(RS_Wins) AS RS_Wins\n",
      "    FROM data_table\n",
      "    GROUP BY Champion\n",
      ") AS subquery\n",
      "ORDER BY Average_RS_Wins DESC\n",
      "LIMIT 5;': no such column: RS_Wins\n",
      "Iteration 6\n",
      "Using tool: calculate\n",
      "Calculate response: SELECT Champion, AVG(RS_Wins) AS Average_RS_Wins\n",
      "FROM (\n",
      "    SELECT Champion, SUM(RS_Wins) AS RS_Wins\n",
      "    FROM data_table\n",
      "    GROUP BY Champion\n",
      ") AS subquery\n",
      "ORDER BY Average_RS_Wins DESC\n",
      "LIMIT 5;\n",
      "Error executing SQL query: Execution failed on sql 'SELECT Champion, AVG(RS_Wins) AS Average_RS_Wins\n",
      "FROM (\n",
      "    SELECT Champion, SUM(RS_Wins) AS RS_Wins\n",
      "    FROM data_table\n",
      "    GROUP BY Champion\n",
      ") AS subquery\n",
      "ORDER BY Average_RS_Wins DESC\n",
      "LIMIT 5;': no such column: RS_Wins\n",
      "Iteration 7\n",
      "Using tool: calculate\n",
      "Calculate response: SELECT Champion, AVG(RS_Wins) AS Average_RS_Wins\n",
      "FROM (\n",
      "    SELECT Champion, SUM(RS_Wins) AS RS_Wins\n",
      "    FROM data_table\n",
      "    GROUP BY Champion\n",
      ") AS subquery\n",
      "ORDER BY Average_RS_Wins DESC\n",
      "LIMIT 5;\n",
      "Error executing SQL query: Execution failed on sql 'SELECT Champion, AVG(RS_Wins) AS Average_RS_Wins\n",
      "FROM (\n",
      "    SELECT Champion, SUM(RS_Wins) AS RS_Wins\n",
      "    FROM data_table\n",
      "    GROUP BY Champion\n",
      ") AS subquery\n",
      "ORDER BY Average_RS_Wins DESC\n",
      "LIMIT 5;': no such column: RS_Wins\n",
      "Iteration 8\n",
      "Using tool: calculate\n",
      "Calculate response: SELECT Champion, AVG(RS_Wins) AS Average_RS_Wins\n",
      "FROM (\n",
      "    SELECT Champion, SUM(RS_Wins) AS RS_Wins\n",
      "    FROM data_table\n",
      "    GROUP BY Champion\n",
      ") AS subquery\n",
      "ORDER BY Average_RS_Wins DESC\n",
      "LIMIT 5;\n",
      "Error executing SQL query: Execution failed on sql 'SELECT Champion, AVG(RS_Wins) AS Average_RS_Wins\n",
      "FROM (\n",
      "    SELECT Champion, SUM(RS_Wins) AS RS_Wins\n",
      "    FROM data_table\n",
      "    GROUP BY Champion\n",
      ") AS subquery\n",
      "ORDER BY Average_RS_Wins DESC\n",
      "LIMIT 5;': no such column: RS_Wins\n",
      "Iteration 9\n",
      "Using tool: calculate\n",
      "Calculate response: SELECT Champion, AVG(RS_Wins) AS Average_RS_Wins\n",
      "FROM (\n",
      "    SELECT Champion, SUM(RS_Wins) AS RS_Wins\n",
      "    FROM data_table\n",
      "    GROUP BY Champion\n",
      ") AS subquery\n",
      "ORDER BY Average_RS_Wins DESC\n",
      "LIMIT 5;\n",
      "Error executing SQL query: Execution failed on sql 'SELECT Champion, AVG(RS_Wins) AS Average_RS_Wins\n",
      "FROM (\n",
      "    SELECT Champion, SUM(RS_Wins) AS RS_Wins\n",
      "    FROM data_table\n",
      "    GROUP BY Champion\n",
      ") AS subquery\n",
      "ORDER BY Average_RS_Wins DESC\n",
      "LIMIT 5;': no such column: RS_Wins\n",
      "Iteration 10\n",
      "Using tool: calculate\n",
      "Calculate response: SELECT Champion, AVG(RS_Wins) AS Average_RS_Wins\n",
      "FROM (\n",
      "    SELECT Champion, SUM(RS_Wins) AS RS_Wins\n",
      "    FROM data_table\n",
      "    GROUP BY Champion\n",
      ") AS subquery\n",
      "ORDER BY Average_RS_Wins DESC\n",
      "LIMIT 5;\n",
      "Error executing SQL query: Execution failed on sql 'SELECT Champion, AVG(RS_Wins) AS Average_RS_Wins\n",
      "FROM (\n",
      "    SELECT Champion, SUM(RS_Wins) AS RS_Wins\n",
      "    FROM data_table\n",
      "    GROUP BY Champion\n",
      ") AS subquery\n",
      "ORDER BY Average_RS_Wins DESC\n",
      "LIMIT 5;': no such column: RS_Wins\n",
      "Iteration 11\n",
      "Using tool: calculate\n",
      "Calculate response: SELECT Champion, AVG(RS_Wins) AS Average_RS_Wins\n",
      "FROM (\n",
      "    SELECT Champion, SUM(RS_Wins) AS RS_Wins\n",
      "    FROM data_table\n",
      "    GROUP BY Champion\n",
      ") AS subquery\n",
      "ORDER BY Average_RS_Wins DESC\n",
      "LIMIT 5;\n",
      "Error executing SQL query: Execution failed on sql 'SELECT Champion, AVG(RS_Wins) AS Average_RS_Wins\n",
      "FROM (\n",
      "    SELECT Champion, SUM(RS_Wins) AS RS_Wins\n",
      "    FROM data_table\n",
      "    GROUP BY Champion\n",
      ") AS subquery\n",
      "ORDER BY Average_RS_Wins DESC\n",
      "LIMIT 5;': no such column: RS_Wins\n",
      "Iteration 12\n",
      "Using tool: calculate\n",
      "Calculate response: SELECT Champion, AVG(RS_Wins) AS Average_RS_Wins\n",
      "FROM (\n",
      "    SELECT Champion, SUM(RS_Wins) AS RS_Wins\n",
      "    FROM data_table\n",
      "    GROUP BY Champion\n",
      ") AS subquery\n",
      "ORDER BY Average_RS_Wins DESC\n",
      "LIMIT 5;\n",
      "Error executing SQL query: Execution failed on sql 'SELECT Champion, AVG(RS_Wins) AS Average_RS_Wins\n",
      "FROM (\n",
      "    SELECT Champion, SUM(RS_Wins) AS RS_Wins\n",
      "    FROM data_table\n",
      "    GROUP BY Champion\n",
      ") AS subquery\n",
      "ORDER BY Average_RS_Wins DESC\n",
      "LIMIT 5;': no such column: RS_Wins\n",
      "Iteration 13\n",
      "Using tool: calculate\n",
      "Calculate response: SELECT Champion, AVG(RS_Wins) AS Average_RS_Wins\n",
      "FROM (\n",
      "    SELECT Champion, SUM(RS_Wins) AS RS_Wins\n",
      "    FROM data_table\n",
      "    GROUP BY Champion\n",
      ") AS subquery\n",
      "ORDER BY Average_RS_Wins DESC\n",
      "LIMIT 5;\n",
      "Error executing SQL query: Execution failed on sql 'SELECT Champion, AVG(RS_Wins) AS Average_RS_Wins\n",
      "FROM (\n",
      "    SELECT Champion, SUM(RS_Wins) AS RS_Wins\n",
      "    FROM data_table\n",
      "    GROUP BY Champion\n",
      ") AS subquery\n",
      "ORDER BY Average_RS_Wins DESC\n",
      "LIMIT 5;': no such column: RS_Wins\n",
      "Iteration 14\n",
      "Using tool: calculate\n",
      "Calculate response: SELECT Champion, AVG(RS_Wins) AS Average_RS_Wins\n",
      "FROM (\n",
      "    SELECT Champion, SUM(RS_Wins) AS RS_Wins\n",
      "    FROM data_table\n",
      "    GROUP BY Champion\n",
      ") AS subquery\n",
      "ORDER BY Average_RS_Wins DESC\n",
      "LIMIT 5;\n",
      "Error executing SQL query: Execution failed on sql 'SELECT Champion, AVG(RS_Wins) AS Average_RS_Wins\n",
      "FROM (\n",
      "    SELECT Champion, SUM(RS_Wins) AS RS_Wins\n",
      "    FROM data_table\n",
      "    GROUP BY Champion\n",
      ") AS subquery\n",
      "ORDER BY Average_RS_Wins DESC\n",
      "LIMIT 5;': no such column: RS_Wins\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[152], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m tool_responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Tool: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tools_responses\u001b[38;5;241m.\u001b[39mitems()])\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# print(feedback_query.format(user_query=user_query, tool_responses=tool_responses))\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m cur_tool \u001b[38;5;241m=\u001b[39m \u001b[43mai_service\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext_upd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeedback_query\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_responses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_responses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools_expl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cur_tool \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe tool has given you a proper response.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\goust\\OneDrive\\Έγγραφα\\EARLY_YEARS\\Projects\\genai_analyst\\generative-ai-app\\analyses\\..\\backend\\app\\services\\ai_service.py:9\u001b[0m, in \u001b[0;36mAIService.process_query\u001b[1;34m(self, context, query)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, context: \u001b[38;5;28mstr\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Use the LLM tool to process the query\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_tool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\goust\\OneDrive\\Έγγραφα\\EARLY_YEARS\\Projects\\genai_analyst\\generative-ai-app\\analyses\\..\\backend\\app\\tools\\llm_tool.py:41\u001b[0m, in \u001b[0;36mLLMTool.generate_response\u001b[1;34m(self, context, query)\u001b[0m\n\u001b[0;32m     34\u001b[0m payload \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m: parameters\n\u001b[0;32m     37\u001b[0m     }\n\u001b[0;32m     40\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl, headers\u001b[38;5;241m=\u001b[39mheaders, json\u001b[38;5;241m=\u001b[39mpayload)\n\u001b[1;32m---> 41\u001b[0m response_text \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response_text\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "first_time_flag = True\n",
    "cur_response = None\n",
    "counter = 0\n",
    "\n",
    "tools_responses = {}\n",
    "\n",
    "data_text = data.to_string(index=False)\n",
    "context_upd = context.format(tools=str(list(tools.keys())))\n",
    "\n",
    "tools_expl = \", \".join([f\"{k}: {v}\" for k, v in tools_explanation.items()])\n",
    "\n",
    "while True: \n",
    "    counter += 1\n",
    "    if first_time_flag:\n",
    "        first_time_flag = False\n",
    "        cur_tool = ai_service.process_query(context_upd, master_query.format(data=data_text, user_query=user_query)).replace(\"'\", \"\")\n",
    "    else:\n",
    "        tool_responses = \", \".join([f\"Iteration {k}: Tool: {v[0]}, Response: {v[1]}\" for k, v in tools_responses.items()])\n",
    "        # print(feedback_query.format(user_query=user_query, tool_responses=tool_responses))\n",
    "        cur_tool = ai_service.process_query(context_upd, feedback_query.format(user_query=user_query, tool_responses=tool_responses, tools=tools_expl)).replace(\"'\", \"\")\n",
    "        \n",
    "        if cur_tool == \"yes\":\n",
    "            print(\"The tool has given you a proper response.\")\n",
    "            final_response = ai_service.process_query(context_upd, response_query.format(user_query=user_query, tool_responses=tool_responses))\n",
    "            print(final_response)\n",
    "            break\n",
    "        else:\n",
    "            cur_tool = cur_tool.strip()\n",
    "    print(f\"Iteration {counter}\")\n",
    "    print(f\"Using tool: {cur_tool}\")\n",
    "    cur_response = tools_funcs[cur_tool](data, user_query, context_upd, ai_service)\n",
    "    tools_responses[str(counter)] = [cur_tool, cur_response]\n",
    "    # print(f\"Response: {cur_response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Iteration 1: Tool: visualize, Response: [{'x': 'month', 'y': 'amount', 'color': 'subsidiary_name', 'title': 'Regional Sales by Subsidiary', 'xaxis_title': 'Month', 'yaxis_title': 'Amount'}], Iteration 2: Tool: get_insights, Response: ['To generate valuable business insights about the given data, I will use the following tools: `validate_data`, `get_insights`, and `visualize`.\\\\n\\\\n**Data Validation:**\\\\nThe provided data appears to be a clean and well-structured dataset with no missing values. The `validate_data` tool confirms that the data is valid and ready for analysis.\\\\n\\\\n**Insights:**\\\\nTo answer the query, I will first calculate the monthly revenue per region. Here are the results:\\\\n\\\\n| Region | Monthly Revenue |\\\\n| --- | --- |\\\\n| North America | $124,500 |\\\\n| Europe | $97,500 |\\\\n| Asia | $112,500 |\\\\n| South America | $137,500 |\\\\n| Africa | $91,500 |\\\\n| Australia | $151,500 |\\\\n\\\\nFrom these results, we can draw the following insights:\\\\n\\\\n1. **Regional Revenue Distribution:** The data shows that North America and Australia have the highest monthly revenue, while Africa has the lowest.\\\\n2. **Regional Growth:** South America has the highest growth rate, with an average monthly revenue increase of 12.5%.\\\\n3. **Regional Concentration:** The data suggests that North America and Europe have a higher concentration of revenue, with 35% and 25% of the total revenue, respectively.\\\\n\\\\n**Visualization:**\\\\nTo visualize the data, I will create a bar chart showing the monthly revenue per region.\\\\n\\\\n[Bar Chart: Monthly Revenue per Region]\\\\n\\\\nThis chart provides a clear visual representation of the regional revenue distribution, allowing for easy comparison and identification of trends.\\\\n\\\\n**Actionable Insights:**\\\\nBased on these insights, the following actions can be taken:\\\\n\\\\n1. **Targeted Marketing:** Focus marketing efforts on North America and Australia, as they have the highest revenue and growth potential.\\\\n2. **Regional Expansion:** Consider expanding operations in South America, as it has the highest growth rate and potential for future revenue growth.\\\\n3. **Regional Optimization:** Analyze and optimize operations in Africa, as it has the lowest revenue and potential for improvement.\\\\n\\\\nThese insights and recommendations provide a solid foundation for business decision-making and strategy development.']\""
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no\\nget_insights'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_service.process_query(context_upd, feedback_query.format(user_query=user_query, tool_responses=tool_responses, tools=str(list(tools.keys())))).replace(\"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'validate_data'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_service.process_query(context_upd, master_query.format(data=data_text, user_query=user_query)).replace(\"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Iteration 1: Tool validate_data, Response: ['No empty values found!', 'No duplicate entries found.']\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\", \".join([f\"Iteration {k}: Tool {v[0]}, Response: {v[1]}\" for k, v in tools_responses.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an experienced data analyst, expert in giving quality information and insights about various data types. I will be giving you a dataset, and you will be providing quality deiliverables. You have the following tools available: {{tools}}. '"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Given the following dataset:      month  amount          subsidiary_name        region\\n2024-01-01  125000               Alpha Corp North America\\n2024-02-01   98000                 Beta Ltd        Europe\\n2024-03-01  112000                Gamma Inc          Asia\\n2024-04-01  134000                Delta LLC South America\\n2024-05-01   89000               Epsilon Co        Africa\\n2024-06-01  145000               Zeta Group     Australia\\n2024-07-01  123000        Theta Enterprises North America\\n2024-08-01   97000            Iota Holdings        Europe\\n2024-09-01  110000               Kappa Corp          Asia\\n2024-10-01  140000               Lambda Ltd South America\\n2024-11-01   87000                   Mu Inc        Africa\\n2024-12-01  150000                   Nu LLC     Australia\\n2024-01-01  128000               Omicron Co North America\\n2024-02-01   95000                 Pi Group        Europe\\n2024-03-01  115000          Rho Enterprises          Asia\\n2024-04-01  137000           Sigma Holdings South America\\n2024-05-01   91000                 Tau Corp        Africa\\n2024-06-01  148000              Upsilon Ltd     Australia\\n2024-07-01  125000                  Phi Inc North America\\n2024-08-01   99000                  Chi LLC        Europe\\n2024-09-01  113000                   Psi Co          Asia\\n2024-10-01  143000              Omega Group South America\\n2024-11-01   88000               Alpha Beta        Africa\\n2024-12-01  152000               Beta Gamma     Australia\\n2024-01-01  130000              Gamma Delta North America\\n2024-02-01   97000            Delta Epsilon        Europe\\n2024-03-01  118000             Epsilon Zeta          Asia\\n2024-04-01  139000               Zeta Theta South America\\n2024-05-01   93000               Theta Iota        Africa\\n2024-06-01  150000               Iota Kappa     Australia\\n2024-07-01  127000             Kappa Lambda North America\\n2024-08-01  101000                Lambda Mu        Europe\\n2024-09-01  116000                    Mu Nu          Asia\\n2024-10-01  145000                    Nu Xi South America\\n2024-11-01   89000               Xi Omicron        Africa\\n2024-12-01  155000               Omicron Pi     Australia\\n2024-01-01  132000                  Phi Rho North America\\n2024-02-01   99000                Rho Sigma        Europe\\n2024-03-01  120000                Sigma Tau          Asia\\n2024-04-01  141000              Tau Upsilon South America\\n2024-05-01   95000              Upsilon Phi        Africa\\n2024-06-01  153000                  Phi Chi     Australia\\n2024-07-01  129000                  Chi Psi North America\\n2024-08-01  103000                Psi Omega        Europe\\n2024-09-01  119000              Omega Alpha          Asia\\n2024-10-01  147000         Alpha Beta Gamma South America\\n2024-11-01   91000         Beta Gamma Delta        Africa\\n2024-12-01  157000      Gamma Delta Epsilon     Australia\\n2024-01-01  134000       Delta Epsilon Zeta North America\\n2024-02-01  101000       Epsilon Zeta Theta        Europe\\n2024-03-01  122000          Zeta Theta Iota          Asia\\n2024-04-01  143000         Theta Iota Kappa South America\\n2024-05-01   97000        Iota Kappa Lambda        Africa\\n2024-06-01  155000          Kappa Lambda Mu     Australia\\n2024-07-01  131000             Lambda Mu Nu North America\\n2024-08-01  105000                 Mu Nu Xi        Europe\\n2024-09-01  123000            Nu Xi Omicron          Asia\\n2024-10-01  149000            Xi Omicron Pi South America\\n2024-11-01   93000           Omicron Pi Rho        Africa\\n2024-12-01  160000            Phi Rho Sigma     Australia\\n2024-01-01  136000        Sigma Tau Upsilon North America\\n2024-02-01  103000          Tau Upsilon Phi        Europe\\n2024-03-01  125000          Upsilon Phi Chi          Asia\\n2024-04-01  145000              Phi Chi Psi South America\\n2024-05-01   99000            Chi Psi Omega        Africa\\n2024-06-01  157000          Psi Omega Alpha     Australia\\n2024-07-01  133000         Omega Alpha Beta North America\\n2024-08-01  107000   Alpha Beta Gamma Delta        Europe\\n2024-09-01  126000 Beta Gamma Delta Epsilon          Asia\\n2024-10-01  151000 Gamma Delta Epsilon Zeta South America\\n2024-11-01   95000 Delta Epsilon Zeta Theta        Africa\\n2024-12-01  162000  Epsilon Zeta Theta Iota     Australia, a user query I want you to validate the dataset, and tell me What is the monthly revenue per region?, I want you to decide which tool to use in order to answer the user query. Print only the name of the tool and nothing else. '"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_query.format(tools=tools, data=data_text, user_query=user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Given the following dataset:      month  amount          subsidiary_name        region\\n2024-01-01  125000               Alpha Corp North America\\n2024-02-01   98000                 Beta Ltd        Europe\\n2024-03-01  112000                Gamma Inc          Asia\\n2024-04-01  134000                Delta LLC South America\\n2024-05-01   89000               Epsilon Co        Africa\\n2024-06-01  145000               Zeta Group     Australia\\n2024-07-01  123000        Theta Enterprises North America\\n2024-08-01   97000            Iota Holdings        Europe\\n2024-09-01  110000               Kappa Corp          Asia\\n2024-10-01  140000               Lambda Ltd South America\\n2024-11-01   87000                   Mu Inc        Africa\\n2024-12-01  150000                   Nu LLC     Australia\\n2024-01-01  128000               Omicron Co North America\\n2024-02-01   95000                 Pi Group        Europe\\n2024-03-01  115000          Rho Enterprises          Asia\\n2024-04-01  137000           Sigma Holdings South America\\n2024-05-01   91000                 Tau Corp        Africa\\n2024-06-01  148000              Upsilon Ltd     Australia\\n2024-07-01  125000                  Phi Inc North America\\n2024-08-01   99000                  Chi LLC        Europe\\n2024-09-01  113000                   Psi Co          Asia\\n2024-10-01  143000              Omega Group South America\\n2024-11-01   88000               Alpha Beta        Africa\\n2024-12-01  152000               Beta Gamma     Australia\\n2024-01-01  130000              Gamma Delta North America\\n2024-02-01   97000            Delta Epsilon        Europe\\n2024-03-01  118000             Epsilon Zeta          Asia\\n2024-04-01  139000               Zeta Theta South America\\n2024-05-01   93000               Theta Iota        Africa\\n2024-06-01  150000               Iota Kappa     Australia\\n2024-07-01  127000             Kappa Lambda North America\\n2024-08-01  101000                Lambda Mu        Europe\\n2024-09-01  116000                    Mu Nu          Asia\\n2024-10-01  145000                    Nu Xi South America\\n2024-11-01   89000               Xi Omicron        Africa\\n2024-12-01  155000               Omicron Pi     Australia\\n2024-01-01  132000                  Phi Rho North America\\n2024-02-01   99000                Rho Sigma        Europe\\n2024-03-01  120000                Sigma Tau          Asia\\n2024-04-01  141000              Tau Upsilon South America\\n2024-05-01   95000              Upsilon Phi        Africa\\n2024-06-01  153000                  Phi Chi     Australia\\n2024-07-01  129000                  Chi Psi North America\\n2024-08-01  103000                Psi Omega        Europe\\n2024-09-01  119000              Omega Alpha          Asia\\n2024-10-01  147000         Alpha Beta Gamma South America\\n2024-11-01   91000         Beta Gamma Delta        Africa\\n2024-12-01  157000      Gamma Delta Epsilon     Australia\\n2024-01-01  134000       Delta Epsilon Zeta North America\\n2024-02-01  101000       Epsilon Zeta Theta        Europe\\n2024-03-01  122000          Zeta Theta Iota          Asia\\n2024-04-01  143000         Theta Iota Kappa South America\\n2024-05-01   97000        Iota Kappa Lambda        Africa\\n2024-06-01  155000          Kappa Lambda Mu     Australia\\n2024-07-01  131000             Lambda Mu Nu North America\\n2024-08-01  105000                 Mu Nu Xi        Europe\\n2024-09-01  123000            Nu Xi Omicron          Asia\\n2024-10-01  149000            Xi Omicron Pi South America\\n2024-11-01   93000           Omicron Pi Rho        Africa\\n2024-12-01  160000            Phi Rho Sigma     Australia\\n2024-01-01  136000        Sigma Tau Upsilon North America\\n2024-02-01  103000          Tau Upsilon Phi        Europe\\n2024-03-01  125000          Upsilon Phi Chi          Asia\\n2024-04-01  145000              Phi Chi Psi South America\\n2024-05-01   99000            Chi Psi Omega        Africa\\n2024-06-01  157000          Psi Omega Alpha     Australia\\n2024-07-01  133000         Omega Alpha Beta North America\\n2024-08-01  107000   Alpha Beta Gamma Delta        Europe\\n2024-09-01  126000 Beta Gamma Delta Epsilon          Asia\\n2024-10-01  151000 Gamma Delta Epsilon Zeta South America\\n2024-11-01   95000 Delta Epsilon Zeta Theta        Africa\\n2024-12-01  162000  Epsilon Zeta Theta Iota     Australia, a user query I want you to validate the dataset, and tell me What is the monthly revenue per region?, I want you to decide which tool to use in order to answer the user query. Print only the name of the tool and nothing else. '"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_query.format(tools=str(list(tools.keys())), data=data_text, user_query=user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data: NBA Champions and their regular season wins from 1980 onwards\n",
    "data = {\n",
    "    \"Year\": [\n",
    "        1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989,\n",
    "        1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999,\n",
    "        2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009,\n",
    "        2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019,\n",
    "        2020, 2021, 2022\n",
    "    ],\n",
    "    \"Champion\": [\n",
    "        \"Los Angeles Lakers\", \"Boston Celtics\", \"Los Angeles Lakers\", \"Philadelphia 76ers\", \"Boston Celtics\",\n",
    "        \"Los Angeles Lakers\", \"Boston Celtics\", \"Los Angeles Lakers\", \"Los Angeles Lakers\", \"Detroit Pistons\",\n",
    "        \"Detroit Pistons\", \"Chicago Bulls\", \"Chicago Bulls\", \"Chicago Bulls\", \"Houston Rockets\", \"Houston Rockets\",\n",
    "        \"Chicago Bulls\", \"Chicago Bulls\", \"Chicago Bulls\", \"San Antonio Spurs\", \"Los Angeles Lakers\",\n",
    "        \"Los Angeles Lakers\", \"Los Angeles Lakers\", \"San Antonio Spurs\", \"Detroit Pistons\", \"San Antonio Spurs\",\n",
    "        \"Miami Heat\", \"San Antonio Spurs\", \"Boston Celtics\", \"Los Angeles Lakers\", \"Los Angeles Lakers\",\n",
    "        \"Dallas Mavericks\", \"Miami Heat\", \"Miami Heat\", \"San Antonio Spurs\", \"Golden State Warriors\",\n",
    "        \"Cleveland Cavaliers\", \"Golden State Warriors\", \"Golden State Warriors\", \"Toronto Raptors\",\n",
    "        \"Los Angeles Lakers\", \"Milwaukee Bucks\", \"Golden State Warriors\"\n",
    "    ],\n",
    "    \"Regular Season Wins\": [\n",
    "        60, 62, 57, 65, 62, 62, 67, 65, 62, 63, 59, 61, 67, 57, 58, 47, 72, 69, 62, 37,\n",
    "        67, 56, 58, 60, 54, 59, 52, 58, 66, 65, 57, 57, 66, 54, 67, 73, 67, 58, 53, 52,\n",
    "        56, 51, 53\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "nba_champions = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_file_path = \"nba_champions_1980_onwards.csv\"\n",
    "nba_champions.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
